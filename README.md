# ğŸ± HelloKitty â€“ Projet Robot Chat
<img width="810" height="1080" alt="image" src="https://github.com/user-attachments/assets/7b001b46-020f-4a15-b75d-87ee0aa59870" />


### **Les contributeurs :**
-BENJEMAA Aymen
-SOLTANI Ezer
-ROUABAH Serine

## ğŸ“š Table des matiÃ¨res

1. [ğŸ“œ PrÃ©sentation gÃ©nÃ©rale](#-prÃ©sentation-gÃ©nÃ©rale)
2. [ğŸ“ Architecture matÃ©rielle](#-architecture-matÃ©rielle)
   - [ğŸ”Œ SchÃ©ma systÃ¨me](#-schÃ©ma-systÃ¨me)
   - [ğŸ“Œ Pinout STM32](#-pinout-stm32)
   - [ğŸ› ï¸ PCB et design Ã©lectronique](#-pcb-et-design-Ã©lectronique)
3. [ğŸ§© Architecture logicielle](#-architecture-logicielle)
   - [ğŸ§± Couches logicielles](#-couches-logicielles)
   - [ğŸ•’ Fonctionnement des tÃ¢ches FreeRTOS](#-fonctionnement-des-tÃ¢ches-freertos)
   - [ğŸ”„ Synchronisation et prioritÃ©s](#-synchronisation-et-prioritÃ©s)
4. [âš™ï¸ Drivers et HAL](#-drivers-et-hal)
5. [ğŸ¯ StratÃ©gie comportementale](#-stratÃ©gie-comportementale)
6. [ğŸ“Š Tests et validation](#-tests-et-validation)
7. [ğŸ”§ RÃ©sultats et perspectives](#-rÃ©sultats-et-perspectives)

## PrÃ©sentation gÃ©nÃ©rale

HelloKitty est un robot mobile autonome conÃ§u pour Ã©voluer sur une surface plane sans bordure, dans un jeu de poursuite entre plusieurs robots. Le projet sâ€™inscrit dans le cadre du module SystÃ¨mes Ã‰lectroniques AvancÃ©s de lâ€™ENSEA, et vise Ã  couvrir lâ€™ensemble du cycle de dÃ©veloppement embarquÃ©â€¯: de la conception du PCB Ã  lâ€™implÃ©mentation logicielle temps rÃ©el, en passant par la stratÃ©gie comportementale.

Le robot est capable de dÃ©tecter les bords, dâ€™Ã©viter les chutes, de repÃ©rer dâ€™autres robots, et de changer de rÃ´le (chat â†” souris) en fonction des interactions physiques ou visuelles. Le projet met en Å“uvre des capteurs variÃ©s, une architecture logicielle modulaire, et une gestion fine des tÃ¢ches concurrentes via FreeRTOS.

## **Architecture**  
### **SchÃ©ma architectural**  

<img width="927" height="693" alt="schema_projet" src="https://github.com/user-attachments/assets/1b3bfaf2-3be3-44c0-845a-d9920e9071d3" />

#  Architecture du systÃ¨me embarquÃ© STM32G431CBU6

Ce projet repose sur un microcontrÃ´leur **STM32G431CBU6**, intÃ©grant divers capteurs, modules de communication, moteurs et interfaces utilisateur pour crÃ©er un systÃ¨me autonome capable de capter, traiter et agir dans un environnement physique.

##  MicrocontrÃ´leur central
- **STM32G431CBU6** : cÅ“ur du systÃ¨me, gÃ¨re les communications, le traitement des donnÃ©es et le contrÃ´le des pÃ©riphÃ©riques.

## Alimentation
- **Batterie NiMH 7.2V 1.3Ah** : source principale dâ€™Ã©nergie.
- **RÃ©gulateurs de tension** :
  - **MP1475DJ-LF-P** : convertit la tension en **5V**.
  - **BU33SD5WG-TR** : convertit en **3.3V** pour les composants sensibles.

## Capteurs et modules
- **4 capteurs TOF (Time-of-Flight)** : connectÃ©s via **I2C**, pour mesurer les distances.
- **AccÃ©lÃ©romÃ¨tre ADXL343** : connectÃ© en **I2C**, pour dÃ©tecter les mouvements.
- **Module Bluetooth** : communication sans fil via **UART**.
- **Lidar YDLIDAR X2** : capteur de tÃ©lÃ©mÃ©trie, connectÃ© en **UART**.

## Horloge et programmation
- **Quartz 16MHz** : fournit une horloge stable au microcontrÃ´leur.
- **STLink SWD** : interface de programmation et dÃ©bogage.
- **Hclk** : 170Mhz.

## Moteurs et contrÃ´le
- **2 pilotes de moteur ZXBM5210** : reÃ§oivent des signaux **PWM** pour contrÃ´ler les moteurs gauche et droit.
- **Moteurs avec encodeurs** : permettent un retour de position et de vitesse.

## ğŸ–±ï¸ Interface utilisateur
- **LED** : sortie **GPIO**, pour signalisation.
- **Bouton utilisateur** : entrÃ©e **GPIO**, pour interaction manuelle.
- **Bouton reset** : pour redÃ©marrer le systÃ¨me.

## ğŸ”Œ Connexions colorÃ©es
- **Rouge** : lignes dâ€™alimentation **5V**
- **Orange** : lignes **3.3V**
- **Violet** : **I2C**
- **Bleu** : **UART**
- **Vert** : **GPIO**
- **Noir** : **PWM**

---

Ce schÃ©ma illustre lâ€™interconnexion des modules pour un systÃ¨me embarquÃ© intelligent et rÃ©actif.

## Fonctionnement interne du robot

Le robot ne se contente pas dâ€™exÃ©cuter des actions simples : son microcontrÃ´leur coordonne en continu lâ€™ensemble des capteurs, moteurs et modules pour produire un comportement cohÃ©rent et rÃ©actif. Cette section dÃ©crit la logique interne qui permet au systÃ¨me de fonctionner de maniÃ¨re autonome.

### Organisation logicielle
Le logiciel embarquÃ© est structurÃ© en plusieurs tÃ¢ches indÃ©pendantes.  
Chaque tÃ¢che sâ€™occupe dâ€™un domaine prÃ©cis : analyse des distances, lecture des chocs, gestion des moteurs ou encore surveillance de lâ€™environnement.  
Cette organisation Ã©vite quâ€™une opÃ©ration bloque les autres et garantit une rÃ©activitÃ© constante.

### SystÃ¨me de dÃ©cision
Le robot suit une hiÃ©rarchie de prioritÃ©s pour rÃ©agir correctement aux Ã©vÃ©nements :
- **SÃ©curitÃ© immÃ©diate** : arrÃªt ou retrait en cas de danger (vide, obstacle trop proche, choc).
- **Ã‰vitement** : choix de la direction la plus dÃ©gagÃ©e grÃ¢ce aux donnÃ©es du LiDAR.
- **DÃ©placement normal** : progression ou patrouille lorsque lâ€™environnement est stable.

Cette logique empÃªche les comportements incohÃ©rents et permet des rÃ©actions rapides.

### Gestion dynamique des moteurs
Les moteurs sont ajustÃ©s en permanence selon la situation :
- correction de trajectoire,
- adaptation de la vitesse,
- compensation en cas de rÃ©sistance ou de choc.

Le microcontrÃ´leur calcule ces ajustements en temps rÃ©el, tandis que les drivers appliquent les consignes via PWM.

### Fusion des capteurs
Les informations issues des diffÃ©rents capteurs sont combinÃ©es pour obtenir une vision plus fiable de lâ€™environnement :
- les ToF surveillent les bords,
- le LiDAR analyse lâ€™espace autour du robot,
- lâ€™accÃ©lÃ©romÃ¨tre dÃ©tecte les impacts ou blocages.

Cette fusion permet dâ€™anticiper les risques et dâ€™adapter le comportement du robot de maniÃ¨re fluide.

### Ã‰tats internes
Le robot fonctionne comme une machine Ã  Ã©tats, chacun correspondant Ã  un comportement prÃ©cis :
- exploration,
- Ã©vitement,
- collision dÃ©tectÃ©e,
- danger de chute,
- blocage,
- repos.

Chaque Ã©tat dÃ©finit les actions Ã  effectuer et les conditions pour passer Ã  un autre Ã©tat.

### Indicateurs lumineux
Les LEDs servent de retour visuel pour comprendre lâ€™Ã©tat du robot :
- clignotement rapide : alerte,
- clignotement lent : attente,
- lumiÃ¨re fixe : fonctionnement normal.

Elles permettent de diagnostiquer rapidement le comportement du robot sans accÃ©der au code.

### Synchronisation des communications
Les diffÃ©rents protocoles (UART, SPI, I2C) fonctionnent en parallÃ¨le.  
Pour Ã©viter les conflits, les Ã©changes sont cadencÃ©s et certaines lectures sont prioritaires.  
Les interruptions matÃ©rielles assurent la prise en charge immÃ©diate des Ã©vÃ©nements critiques.

---

Cette architecture logicielle permet au robot dâ€™Ãªtre autonome, rÃ©actif et capable de sâ€™adapter en temps rÃ©el Ã  son environnement.


##  Partie Hardware

Cette section dÃ©crit lâ€™architecture matÃ©rielle du robot, ses composants Ã©lectroniques, et les schÃ©mas associÃ©s.
<img width="983" height="564" alt="image" src="https://github.com/user-attachments/assets/481b72ed-1033-43fa-afc0-7f35f21c6596" />


### ğŸ”Œ SchÃ©ma global du systÃ¨me
Le systÃ¨me repose sur un microcontrÃ´leur **STM32G431CBU6** qui coordonne les capteurs, les moteurs, les rÃ©gulateurs et les interfaces utilisateur.


---

### âš™ï¸ MicrocontrÃ´leur et interfaces
Le microcontrÃ´leur est au cÅ“ur du systÃ¨me. Il est connectÃ© :
- aux moteurs via des signaux **PWM** et des entrÃ©es dâ€™encodeurs,
- aux capteurs via **UART**, **I2C**, et **GPIO**,
- Ã  un **STLink/SWD** pour la programmation et le dÃ©bogage.

<img width="1078" height="742" alt="image" src="https://github.com/user-attachments/assets/20eed7e6-8ec8-43f0-a155-ee942acf7542" />


---

### ğŸ”‹ Alimentation et rÃ©gulation
Le robot est alimentÃ© par une batterie **NiMH 7.2V**, rÃ©gulÃ©e en deux tensions :
- **5V** via le rÃ©gulateur **MP1475DJ-LF-P**,
- **3.3V** via le rÃ©gulateur **BU33SD5WG-TR**.

Ces tensions alimentent les moteurs, le microcontrÃ´leur et les capteurs sensibles.

<img width="1013" height="592" alt="image" src="https://github.com/user-attachments/assets/fa8be56d-729b-44cd-a620-70aa3347fee2" />


---

### ğŸ¦¾ Pilotes de moteurs
Chaque moteur est contrÃ´lÃ© par un circuit **ZXBM5210-SP**, avec :
- deux entrÃ©es **PWM** pour la vitesse et la direction,
- deux sorties vers le moteur (Motor+ / Motorâˆ’),
- des entrÃ©es dâ€™encodeurs pour le retour de position.

Chaque moteur dispose de son propre driver et de ses propres signaux.

<img width="570" height="256" alt="image" src="https://github.com/user-attachments/assets/a448f5a5-3051-4683-b620-6901f1f0ada8" />

---

### ğŸ“¡ Capteurs
Le systÃ¨me intÃ¨gre :
- **4 capteurs TOF** pour la dÃ©tection de bordure,
- **1 accÃ©lÃ©romÃ¨tre ADXL343** pour les chocs et mouvements,
- **1 LiDAR YDLIDAR X2** pour la cartographie et lâ€™Ã©vitement,
- **1 module Bluetooth** pour la communication sans fil.

Tous ces capteurs sont connectÃ©s au microcontrÃ´leur via **I2C**, **SPI**, **UART** ou **GPIO**.
<img width="1062" height="625" alt="image" src="https://github.com/user-attachments/assets/a634e98e-761b-432a-a299-bae5318e2d9d" />

---

### ğŸ–±ï¸ Interface utilisateur
Le robot dispose :
- de **LEDs** pour indiquer son Ã©tat (obstacle, marche, pauseâ€¦),
- dâ€™un **bouton utilisateur** pour les interactions manuelles,
- dâ€™un **bouton reset** pour redÃ©marrer le systÃ¨me.
##im

---

### ğŸ§© Organisation des fichiers KiCad
Les schÃ©mas sont rÃ©partis en plusieurs fichiers :
- `pucontrolleur.kicad.sch` : microcontrÃ´leur et interfaces
- `moteur1.kicad.sch` / `moteur2.kicad.sch` : circuits moteurs
- `regulateurs.kicad.sch` : alimentation
- `capteurs.kicad.sch` : capteurs et communication

---

Cette architecture matÃ©rielle permet au robot dâ€™Ãªtre autonome, rÃ©actif et modulaire. Chaque composant est interconnectÃ© pour assurer un fonctionnement fluide et sÃ©curisÃ©.



https://github.com/user-attachments/assets/3a07851f-27b0-4f3c-a773-fe1d66b704f5


